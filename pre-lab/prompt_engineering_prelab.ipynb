{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOcivZVY7i4dDAupQprVxMY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeKUT-DSAIL/DSA-2024-NLP/blob/main/pre-lab/prompt_engineering_prelab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DSA 2024 - NLP Lab Session\n",
        "\n",
        "### Part 3: Prompt Engineering with LLaMA-2\n",
        "\n",
        "Prompt engineering is the discipline of developing and optimising prompts to effectively use large language models (LLMs) to achieve desired outputs for a wide variety of applications, including research. By developing prompt engineering skills, we are enabled to better understand the capabilities and limitations of these LLMs.\n",
        "\n",
        "The key aspects of prompt engineering include, but are not limited to:\n",
        "* Crafting clear prompts: The model's output is significantly affected by the model's output. To get accurate and relevant responses, prompts should be clear, concise, and specific.\n",
        "* Providing context: Prompts that include sufficient context within them help the models understand the background and generate more informed responses. Contexts can involve giving background information, setting the scene, or specifying the desired format of the answer.\n",
        "* Iterative refinement: Prompt engineering is often an iterative process where initial prompts are continuously adjusted and refined to improve the quality of the response.\n",
        "* Instruction precision: Explicity stating what you want from the model can dramatically improve outcomes. Using words like \"list\", \"describe\", etc. help guide the model more effectively.\n",
        "* Balancing length and detail: Although detailed prompts can provide more guidance, overly long prompts tend to confuse the model. Striking a balance between providing enough details and maintaining brevity is important.\n",
        "* Leveraging special tokens: Some models allow the use of special tokens or specific structures to control responses, such as separators or format indicators. `LLaMA-2` is one such model.\n",
        "\n",
        "#### Prerequisites\n",
        "This lab is targeted at Python developers who have some familiarity with LLMs, such as by using ChatGPT or Gemini, but have limited experience in working with LLMs in a programmatic way.\n",
        "\n",
        "If you're familiar with the underpinnings of LLMs, you'll have a slight advantage. However, familiarity with basic Python and a basic understanding of LLMs will be sufficient to help you get a lot out of this course.\n",
        "\n",
        "For this lab, we shall be working with the `LLaMA-2` model available at [HuggingFace](https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ). In order to download this model for use in this notebook, you will need to install the [transformers](https://pypi.org/project/transformers/) package. Not to worry, the steps for installing it are baked into this Colab notebook and you will not need to take any extra steps, except if you choose to run the notebook locally.\n",
        "\n",
        "\n",
        "#### Learning Objectives\n",
        "1. Use a `transformers` pipeline to generate responses from a LLaMA-2 LLM.\n",
        "2. Iteratively write precise prompts to get the desired output from the LLM.\n",
        "3. Work with the LLaMA-2 prompt template to perform instruction fine-tuning.\n",
        "4. Use LLaMA-2 to generate JSON data for potential use in downstream processing tasks\n",
        "\n",
        "\n",
        "Let's get cracking."
      ],
      "metadata": {
        "id": "PfARh5EqRG91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup\n",
        "\n",
        "Let's get started by setting up our environment by installing the `transformers` package and importing the necessary packages.\n",
        "\n",
        "Remember to change the runtime type on Colab to GPU by following these steps:\n",
        "* On the menu bar, click on `Runtime`.\n",
        "* Click `change runtime type`.\n",
        "* In the `Hardware accelerator` radio options, select `T4 GPU`.\n",
        "* Click `save`.\n",
        "\n"
      ],
      "metadata": {
        "id": "bsLoqFco76Zt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ6R91YRE6HH"
      },
      "outputs": [],
      "source": [
        "! pip install transformers\n",
        "! pip install accelerate\n",
        "! pip install optimum\n",
        "! pip install auto-gptq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import accelerate"
      ],
      "metadata": {
        "id": "Oz7NrNLzQ_eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's ignore the warrnings to keep the cell outputs nice and clean"
      ],
      "metadata": {
        "id": "aLs4c24DEhG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "warnings.warn = warn"
      ],
      "metadata": {
        "id": "QE86BDXrEXXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create LLaMA-2 Pipeline"
      ],
      "metadata": {
        "id": "2MoXLGvz-NT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
        "llama_pipe = pipeline(\"text-generation\", model=model, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "H1li-Dgu-GMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A Helper Function"
      ],
      "metadata": {
        "id": "qHGaKnrkBEj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, max_length=1024, pipe=llama_pipe, **kwargs):\n",
        "    \"\"\"\n",
        "    This function takes a prompt and passes it to the model e.g. LLaMA and returnss the response from the model\n",
        "\n",
        "    Parameters:\n",
        "    @param prompt (str): The input text prompt to generate a response for.\n",
        "    @param max_length (int): The maximum length, in tokens, of the generated response.\n",
        "    @param pipe (callable): The model's pipeline function used for generation.\n",
        "    @param **kwargs: Additional keyword arguments that are passed to the pipeline function.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated text response from the model, trimmed of leading and trailing whitespace.\n",
        "\n",
        "    Example usage:\n",
        "    ```\n",
        "    prompt_text = \"Explain the theory of relativity.\"\n",
        "    response = generate(prompt_text, max_length=512, pipe=my_custom_pipeline, temperature=0.7)\n",
        "    print(response)\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    def_kwargs = dict(return_full_text=False, return_dict=False)\n",
        "    response = pipe(prompt.strip(), max_length=max_length, **kwargs, **def_kwargs)\n",
        "    return response[0]['generated_text'].strip()"
      ],
      "metadata": {
        "id": "ha2P7klTDpwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First prompt: Capital of California**\n",
        "\n",
        "We shall begin with a very simple prompt and pass it to the `LLaMA-2` model. The desired outcome is that the model responds to us with only the name of the capital of California, which is *Sacramento*, with nothing else in the response."
      ],
      "metadata": {
        "id": "6KzuS-u0BwDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the capital of California?\"\n",
        "\n",
        "print(generate(prompt))"
      ],
      "metadata": {
        "id": "C9WtHw6eDtUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model responds back with additional helpful information about the city of Sacramento. We are not interested in this additional information. We want the name of the city and no additional context, so let's craft a prompt that is more **specific**."
      ],
      "metadata": {
        "id": "iEZXfA3UBzjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the capital of California? Only answer this question and do so in as few a words as possible.\"\n",
        "\n",
        "print(generate(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK-RTpk9ECVx",
        "outputId": "ccae6d32-330d-448b-b8a2-6b110d1ef75a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Sacramento\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That was a better response, but we still get a leading `Answer:` in the reponse. We can prevent this behaviour by providing the model with the **cue** `Answer:`. Doing this can prevent the model from providing the text itself."
      ],
      "metadata": {
        "id": "kdd9r2GsCO_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the capital of California? Only answer this question and do so in as few a words as possible. Answer: \"\n",
        "\n",
        "print(generate(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIXB5POMEHWO",
        "outputId": "905a913a-15b0-421c-97cf-ad8eb24056e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sacramento.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second prompt: Vowels in 'Sacramento'**\n",
        "\n",
        "In this part of the notebook, we ask the model to do somethig more complicated i.e., tell us the vowes found in the name of the capital of California.\n",
        "\n",
        "We know the correct answer is S**a**cr**a**m**e**nt**o** -> **aaeo** -> **aeo**. Notice that, in order to arrive at the correct answer, you probably had to perform the task in multiple steps."
      ],
      "metadata": {
        "id": "CbNBGoNwCxHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Tell me the vowels in the capital of California.\"\n",
        "\n",
        "print(generate(prompt))"
      ],
      "metadata": {
        "id": "JNs2IlvaC5aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When LLMs are faced with the need to reason in a way that requires multiple steps, it is often helpful to craft a prompt instructing the model to perform multiple intermediary steps, like asking it to show its working. This technique is often described as giving the model **\"time to think\"**.\n",
        "\n",
        "Let's now craft a new prompt asking the model to take the intermediate step of identifying the capital of Kenya before identifying the vowels in it."
      ],
      "metadata": {
        "id": "Zoiw-IuND_kK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Tell me the capital of California, and then tell me all the vowels in it.\"\n",
        "\n",
        "print(generate(prompt))"
      ],
      "metadata": {
        "id": "Mqf5HGUTE-hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that giving the model **time to think** made a big difference. Armed with this new technique, let's ask the model to do something more complicated - tell the vowels in the name of the capital of Kenya in reverse alphabetical order.\n",
        "\n",
        "The correct answer is S**a**cr**a**m**e**nt**o** -> **aeo** -> **oea**"
      ],
      "metadata": {
        "id": "UT_QXRB4FEeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Tell me the vowels in the capital of California in reverse alphabetical order?\"\n",
        "\n",
        "print(generate(prompt))"
      ],
      "metadata": {
        "id": "jfdxW1lxFkac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obviously, we did not give the model **time to think**. Let's ask it to break the task down to intermediate steps and show its work."
      ],
      "metadata": {
        "id": "iX6V6Iz1FwXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Tell me the capital of California, and then tell me all the vowels in it, then tell me the vowels in reverse-alphabetical order.\"\n",
        "\n",
        "print(generate(prompt))"
      ],
      "metadata": {
        "id": "Stws0XT8F_m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise\n",
        "Ask the model to perform the product of 37 and 48 and then iteratively develop a concise prompt which results in the correct response from the model.\n",
        "\n",
        "**Note**: LLMs aren't the best tools for performing math. So, be sure to consider how you can be **precise** in your prompt and also allow the model **time to think**.\n",
        "\n",
        "If you get stuck, we have provided a solution further down."
      ],
      "metadata": {
        "id": "iVCxSmP_GLOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The right answer\n",
        "37*48"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI9xVmKUHGQb",
        "outputId": "7c0b22a3-66c1-49e6-b8b7-0beb6e692c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1776"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"37x48\"\n",
        "\n",
        "print(generate(prompt))"
      ],
      "metadata": {
        "id": "wbFa9lHVHKuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Your work here"
      ],
      "metadata": {
        "id": "lMAWPhT6HYgz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sWZHtuvcHX9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "Click me to see the solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "prompt = \"Calculate the product of 37 and 48. Use the steps typical of long multiplication and show your work.\"\n",
        "print(generate(prompt))\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "tdvTEYGhHdyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Review\n",
        "\n",
        "In this notebook, you have learnt the following key concepts:\n",
        "\n",
        "* **Precise**: Being as specific and explicit as necessary to guide the response of an LLM.\n",
        "* **Cue**: A phrase to conclude your prompt to guides its response, and often to prevent it from including the cue itself in its response.\n",
        "* **\"Time to think\"**: A quality in prompts that supports LLM responses (often requiring calculation) by asking for the model to take multiple steps and show its work."
      ],
      "metadata": {
        "id": "_fKJUsolJES0"
      }
    }
  ]
}