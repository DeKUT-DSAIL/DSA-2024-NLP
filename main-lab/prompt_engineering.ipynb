{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DeKUT-DSAIL/DSA-2024-NLP/blob/main/main-lab/prompt_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfARh5EqRG91"
   },
   "source": [
    "## DSA 2024 - NLP Lab Session\n",
    "\n",
    "### Part 3: Prompt Engineering with LLaMA-2\n",
    "\n",
    "Prompt engineering is the discipline of developing and optimising prompts to effectively use large language models (LLMs) to achieve desired outputs for a wide variety of applications, including research. By developing prompt engineering skills, we are enabled to better understand the capabilities and limitations of these LLMs.\n",
    "\n",
    "The key aspects of prompt engineering include, but are not limited to:\n",
    "* Crafting clear prompts: The model's output is significantly affected by the model's output. To get accurate and relevant responses, prompts should be clear, concise, and specific.\n",
    "* Providing context: Prompts that include sufficient context within them help the models understand the background and generate more informed responses. Contexts can involve giving background information, setting the scene, or specifying the desired format of the answer.\n",
    "* Iterative refinement: Prompt engineering is often an iterative process where initial prompts are continuously adjusted and refined to improve the quality of the response.\n",
    "* Instruction precision: Explicity stating what you want from the model can dramatically improve outcomes. Using words like \"list\", \"describe\", etc. help guide the model more effectively.\n",
    "* Balancing length and detail: Although detailed prompts can provide more guidance, overly long prompts tend to confuse the model. Striking a balance between providing enough details and maintaining brevity is important.\n",
    "* Leveraging special tokens: Some models allow the use of special tokens or specific structures to control responses, such as separators or format indicators. `LLaMA-2` is one such model.\n",
    "\n",
    "#### Prerequisites\n",
    "This lab is targeted at Python developers who have some familiarity with LLMs, such as by using ChatGPT or Gemini, but have limited experience in working with LLMs in a programmatic way.\n",
    "\n",
    "If you're familiar with the underpinnings of LLMs, you'll have a slight advantage. However, familiarity with basic Python and a basic understanding of LLMs will be sufficient to help you get a lot out of this course.\n",
    "\n",
    "For this lab, we shall be working with the `LLaMA-2` model available at [HuggingFace](https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ). In order to download this model for use in this notebook, you will need to install the [transformers](https://pypi.org/project/transformers/) package. Not to worry, the steps for installing it are baked into this Colab notebook and you will not need to take any extra steps, except if you choose to run the notebook locally.\n",
    "\n",
    "**Note:** If you have trouble getting the code to work or if it's taking too long, you can consider copy-pasting the prompts on ChatGPT web GUI from OpenAI - [link here](https://chat.openai.com). You can create a free account or login with your Google credentials. \n",
    "\n",
    "#### Learning Objectives\n",
    "1. Use a `transformers` pipeline to generate responses from a LLaMA-2 LLM.\n",
    "2. Iteratively write precise prompts to get the desired output from the LLM.\n",
    "3. Work with the LLaMA-2 prompt template to perform instruction fine-tuning.\n",
    "4. Use LLaMA-2 to generate JSON data for potential use in downstream processing tasks\n",
    "\n",
    "\n",
    "Let's get cracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAg7J3UQlTji"
   },
   "outputs": [],
   "source": [
    "# Run this cell to prevent code cell outputs from overflowing.\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "    display(HTML('''\n",
    "        <style>\n",
    "            pre {\n",
    "                white-space: pre-wrap;\n",
    "            }\n",
    "        </style>\n",
    "    '''))\n",
    "\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLs4c24DEhG3"
   },
   "source": [
    "First, let's ignore the warrnings to keep the cell outputs nice and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "M7sjezxCFPTb",
    "outputId": "4d59f76c-a50e-4966-b1c0-8d076cbb6fb9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "QE86BDXrEXXa",
    "outputId": "f2d46d44-b78a-4f00-88e0-adb9ed59201c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsLoqFco76Zt"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Let's get started by setting up our environment by installing the `transformers` package and importing the necessary packages.\n",
    "\n",
    "Remember to change the runtime type on Colab to GPU by following these steps:\n",
    "* On the menu bar, click on `Runtime`.\n",
    "* Click `change runtime type`.\n",
    "* In the `Hardware accelerator` radio options, select `T4 GPU`.\n",
    "* Click `save`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "XQ6R91YRE6HH",
    "outputId": "012b38ed-c273-446a-9627-3cdf8367a9a1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip install accelerate\n",
    "! pip install optimum\n",
    "! pip install auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "7hclY4uCl78S",
    "outputId": "fb93ca89-cd8d-4301-d5c3-4bce9b3dcdcd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.\n",
      "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA extension not installed.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import auto_gptq\n",
    "import accelerate\n",
    "import optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Oz7NrNLzQ_eM",
    "outputId": "8996cc30-3c4a-4b13-fed9-8a728053de2f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "# import accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MoXLGvz-NT_"
   },
   "source": [
    "## 1. Task One: Iterative Prompt Refinement\n",
    "### Create LLaMA-2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "H1li-Dgu-GMR",
    "outputId": "621213f2-6773-4dd9-8737-c868aa66b4b0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "llama_pipe = pipeline(\"text-generation\", model=model, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHGaKnrkBEj8"
   },
   "source": [
    "#### A Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ha2P7klTDpwW",
    "outputId": "6d79ba68-d33d-4d76-d034-d833196e7147"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate(prompt, max_length=1024, pipe=llama_pipe, **kwargs):\n",
    "    \"\"\"\n",
    "    This function takes a prompt and passes it to the model e.g. LLaMA and returnss the response from the model\n",
    "\n",
    "    Parameters:\n",
    "    @param prompt (str): The input text prompt to generate a response for.\n",
    "    @param max_length (int): The maximum length, in tokens, of the generated response.\n",
    "    @param pipe (callable): The model's pipeline function used for generation.\n",
    "    @param **kwargs: Additional keyword arguments that are passed to the pipeline function.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated text response from the model, trimmed of leading and trailing whitespace.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    prompt_text = \"Explain the theory of relativity.\"\n",
    "    response = generate(prompt_text, max_length=512, pipe=my_custom_pipeline, temperature=0.7)\n",
    "    print(response)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def_kwargs = dict(return_full_text=False, return_dict=False)\n",
    "    response = pipe(prompt.strip(), max_length=max_length,  truncation=True, **kwargs, **def_kwargs)\n",
    "    return response[0]['generated_text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KzuS-u0BwDg"
   },
   "source": [
    "**First prompt: Capital of California**\n",
    "\n",
    "We shall begin with a very simple prompt and pass it to the `LLaMA-2` model. The desired outcome is that the model responds to us with only the name of the capital of California, which is *Sacramento*, with nothing else in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "C9WtHw6eDtUu",
    "outputId": "cc318744-35dc-48f6-9300-50c3cca654fa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The capital of California is Sacramento.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of California?\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEZXfA3UBzjt"
   },
   "source": [
    "The model responds back with additional helpful information about the city of Sacramento. We are not interested in this additional information. We want the name of the city and no additional context, so let's craft a prompt that is more **specific**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pK-RTpk9ECVx",
    "outputId": "075f24ba-98f9-4cee-aa09-caa236db1cb2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Sacramento\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of California? Only answer this question and do so in as few a words as possible.\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdd9r2GsCO_A"
   },
   "source": [
    "That was a better response, but we still get a leading `Answer:` in the reponse. We can prevent this behaviour by providing the model with the **cue** `Answer:`. Doing this can prevent the model from providing the text itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xIXB5POMEHWO",
    "outputId": "1a284925-55b6-4ef8-d4bd-e8afdd8559d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sacramento.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of California? Only answer this question and do so in as few a words as possible. Answer: \"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbNBGoNwCxHm"
   },
   "source": [
    "**Second prompt: Vowels in 'Sacramento'**\n",
    "\n",
    "In this part of the notebook, we ask the model to do somethig more complicated i.e., tell us the vowes found in the name of the capital of California.\n",
    "\n",
    "We know the correct answer is S**a**cr**a**m**e**nt**o** -> **aaeo** -> **aeo**. Notice that, in order to arrive at the correct answer, you probably had to perform the task in multiple steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "JNs2IlvaC5aC",
    "outputId": "0b07c231-9cd7-4992-a7b7-d43d554f49d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The vowels in the capital of California are \"A\" and \"E\".\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me the vowels in the capital of California.\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zoiw-IuND_kK"
   },
   "source": [
    "When LLMs are faced with the need to reason in a way that requires multiple steps, it is often helpful to craft a prompt instructing the model to perform multiple intermediary steps, like asking it to show its working. This technique is often described as giving the model **\"time to think\"**.\n",
    "\n",
    "Let's now craft a new prompt asking the model to take the intermediate step of identifying the capital of Kenya before identifying the vowels in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "Mqf5HGUTE-hL",
    "outputId": "a43db8d3-b3e7-4a8a-e2e0-a14abdc8271c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll start: the capital of California is Sacramento.\n",
      "\n",
      "Now, the vowels in Sacramento are a, e, and o.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me the capital of California, and then tell me all the vowels in it.\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UT_QXRB4FEeD"
   },
   "source": [
    "We can see that giving the model **time to think** made a big difference. Armed with this new technique, let's ask the model to do something more complicated - tell the vowels in the name of the capital of Kenya in reverse alphabetical order.\n",
    "\n",
    "The correct answer is S**a**cr**a**m**e**nt**o** -> **aeo** -> **oea**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jfdxW1lxFkac",
    "outputId": "d462c35d-1b8d-4e25-cda5-9441bebb088b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm thinking... uh... oh, I know this one! The vowels in the capital of California in reverse alphabetical order are... e-a-o-u!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me the vowels in the capital of California in reverse alphabetical order?\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iX6V6Iz1FwXL"
   },
   "source": [
    "Obviously, we did not give the model **time to think**. Let's ask it to break the task down to intermediate steps and show its work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "Stws0XT8F_m6",
    "outputId": "1fb954a9-04eb-4049-b064-84676bd4f3f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not sure if you're aware, but the capital of California is Sacramento.\n",
      "\n",
      "Now, let's get to the vowels. The vowels in Sacramento are A, E, and O.\n",
      "\n",
      "Now, let's put them in reverse alphabetical order. The reverse alphabetical order of the vowels in Sacramento is O, E, and A.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me the capital of California, and then tell me all the vowels in it, then tell me the vowels in reverse-alphabetical order.\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fKJUsolJES0"
   },
   "source": [
    "## 2. Task Two: Using the LLaMA-2 Prompt Template\n",
    "In this section, we shall use the LLaMA-2 model to perform sentiment analysis on textual reviews and generate data for downstream tasks. You will learn how to use **few-shot learning** to improve the accuracy of the model output by providing it with instructive examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsYhdOtdNZ97"
   },
   "source": [
    "### Data - Nyota Ndogo Bike Reviews\n",
    "\n",
    "The following are customer reviews for the Nyota Ndogo bicycle offered by a fictitious bicycle company known as Nyota Bicycles. We shall be asking the LLM to **sentiment analysis** of these reviews as well as generate data for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "t7jb0T6qJtvI",
    "outputId": "255c94e5-567d-44b4-a96a-e7016d3619d9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "review = \"\"\"\n",
    "I recently purchased the Nyota Ndogo from Nyota Bicycles, and I've been thoroughly impressed. \\\n",
    "The ride is smooth and it handles urban terrains with ease. \\\n",
    "However, I did find the seat a bit uncomfortable for longer rides. \\\n",
    "Also, the color options could be better. Despite these minor drawbacks, \\\n",
    "the build quality and the performance of the bike are commendable. It's a good value for the money.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "_T0LNhKbOooH",
    "outputId": "3263e846-2ff3-4eee-e1fa-7da2a0b3bd62"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "review_negative = \"\"\"\n",
    "Got the Nyota Ndogo last week, and I'm a bit disappointed. \\\n",
    "The brakes are not as responsive as I'd like and the gears often get stuck. \\\n",
    "The design is good but performance-wise, it leaves much to be desired.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "HOCkYP5VO2-S",
    "outputId": "60b852c1-1fbc-4fd4-c74d-1af44318ff47"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "review_positive = \"\"\"\n",
    "I recently purchased the Nyota Ndogo from Nyota Bicycles, and I've been thoroughly impressed. \\\n",
    "The ride is smooth and it handles urban terrains with ease. \\\n",
    "The seat was very comfortable for longer rides and the color options were great. \\\n",
    "The build quality and the performance of the bike are commendable. It's a good value for the money.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNjMlWoNPAu7"
   },
   "source": [
    "#### Sentiment Analysis\n",
    "\n",
    "We will ask the model to perform sentiment analysis by telling us the overall sentiment of one of the reviews. The model should output a response with a single word telling, either `positive`, `negative`, or `neutral`.\n",
    "\n",
    "**NOTE:** The following code cell will take too much time (~20 mins) to run than we have for this lab. So, leave commented out and then you can run it at your own free time. For now, just know that the model's output will be a bunch of **junk**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "abHj90ulPk_m",
    "outputId": "245cd551-38d5-46a4-da82-418ac92d739c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prompt = f\"\"\"\n",
    "# What is the overall sentiment of {review}\n",
    "# \"\"\"\n",
    "\n",
    "# print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URwMu9GBPodQ"
   },
   "source": [
    "It's not at all clear why the model gave us such junk as a response to the prompt above, but that is why we should work on prompts iteratively and **precision** when crafting prompts is required. Let's add a `?` to the end of the prompt, which should make more clear to the model that we are asking it a question we are hoping to get a response to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "rtD3lYJUQGAR",
    "outputId": "198fde45-b868-4f6b-f557-45b448f53d9e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall sentiment of the text is positive. The reviewer mentions that they are \"thoroughly impressed\" with the bike and that it handles urban terrains with ease. However, they also mention a few minor drawbacks, such as the seat being uncomfortable for longer rides and the color options being limited. Despite these issues, the reviewer concludes that the bike is a good value for the money.\n"
     ]
    }
   ],
   "source": [
    "# this cell will take a little over 2 minutes to run\n",
    "prompt = f\"\"\"\n",
    "What is the overall sentiment of {review}?\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYKbNtahQJcG"
   },
   "source": [
    "As you can see, the `?` made a huge difference! This example reminds us that minor tweaks to prompts can sometimes lead to drastic changes in the model's responses.\n",
    "\n",
    "Given that our goal here is to get a single word response back from the model, let's make the prompt more **specific** about the response we would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "YbDDswZEQhuk",
    "outputId": "25c41ab7-d2a2-4d66-a421-4aea1862ed5c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This review is positive. The reviewer mentions that the ride is smooth, the bike handles urban terrains with ease, and the build quality is commendable. They also mention that the bike is a good value for the money. However, they do mention that the seat can be uncomfortable for longer rides and that the color options could be better. Overall, the review is positive because the reviewer has more positive comments than negative ones.\n"
     ]
    }
   ],
   "source": [
    "# this cell will take a little over 2 minutes to run\n",
    "prompt = f\"\"\"\n",
    "What is the overall sentiment of this review {review}?\n",
    "\n",
    "Choose one of \"positive\", \"negative\", or \"neutral\".\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jrzSl38Qlc9"
   },
   "source": [
    "Although the model's reponse is helpful, it is more than just the single word response we are aiming for. Let's add a **cue** to the prompt as we have done before and check its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "TqdPF4LtQ_hG",
    "outputId": "7a5503bd-82e3-4d88-e90b-198bc646e753"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "# adding the cue made the model faster at giving its output\n",
    "prompt = f\"\"\"\n",
    "What is the overall sentiment of this review {review}?\n",
    "\n",
    "Choose one of \"positive\", \"negative\", or \"neutral\".\n",
    "Overall Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc37tes7REbp"
   },
   "source": [
    "That's much better. But we expect this review to be categorised as `neutral` rather than `positive`. Here is the prompt again for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "3Fi1EgmQRcRO",
    "outputId": "1c695a9a-cf11-4225-c52a-b31925e58f00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "review = \"\"\"\n",
    "I recently purchased the Nyota Ndogo from Nyota Bicycles, and I've been thoroughly impressed. \\\n",
    "The ride is smooth and it handles urban terrains with ease. \\\n",
    "However, I did find the seat a bit uncomfortable for longer rides. \\\n",
    "Also, the color options could be better. Despite these minor drawbacks, \\\n",
    "the build quality and the performance of the bike are commendable. It's a good value for the money.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7abdKfskRfEq"
   },
   "source": [
    "Let's make a minor change to the prompt that will lead to drastic changes in the model's response. We do so by modifying the options that model has to choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "4DKXU4tXRuoY",
    "outputId": "aa84fbea-d468-4717-9e46-24c2a1d0951c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral. The reviewer has mixed feelings about the bike. They like the smooth ride and the bike's ability to handle urban terrain, but they found the seat uncomfortable and the color options limited.\n"
     ]
    }
   ],
   "source": [
    "# this cell will take a little over 1 minute to run\n",
    "prompt = f\"\"\"\n",
    "What is the overall sentiment of this review {review}?\n",
    "\n",
    "Choose one of \"neutral\", \"negative\", or \"positive\".\n",
    "Overall Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o04V7l2MRzJc"
   },
   "source": [
    "Before we can conclude this experiment, let's change the order of the options one more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "XlFetLS6R9ru",
    "outputId": "0c47d418-d06e-4342-c1c0-5bb7b8372e5a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the overall sentiment of this review {review}?\n",
    "\n",
    "Choose one of \"negative\", \"neutral\", or \"positive\".\n",
    "Overall Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMiigSyUSAI0"
   },
   "source": [
    "Three different orders of the options and three different responses. That's not very helpful. We conclude that our prompt does not currently give us confidence that we will get meaningful responses from the model.\n",
    "\n",
    "In order to get more reliable, trustworthy responses, let's turn our attention to an important technique, **few-shot learning**, which will allow us to provide instructive examples to the model about how it ought to behave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fduId11jTzRp"
   },
   "source": [
    "#### **Few-Shot Learning - Providing Examples**\n",
    "\n",
    "Depending on the number of examples given, this technique can be called **one-shot learning**, **two-shot learning**, **three-shot learning**, or **many-shot learning**. A **shot** is an example prompt/response pair provided to the model to help guide its behaviour.\n",
    "\n",
    "These shots are typically prepended to the main prompt we wish the model to generate a response for. Depending on the model being used, there are specific ways to format our shots that will help the model understand that what we are providing it are prompt/response examples.\n",
    "\n",
    "On Hugging Face, there are models of the `-chat` variant, such as the `Llama-2-13b-chat` that we have been using. These models have had additional training on top of the base models e.g. `Llama-2-13b` that makes them better at following instructions in support of their use in chat applications. They are said to have been **instruction fine-tuned**.\n",
    "\n",
    "Depending on the model, the prommpt/response pairs will be formatted in different ways using a training template known as a **prompt template**, typically found in the model's documentation. Here is simplified version of the of the **Llama-2 prompt template** we shall be using shortly.\n",
    "\n",
    "```python\n",
    "<s>[INST] {{ user_msg_1 }} [/INST] {{ model_answr_1 }} </s>\n",
    "```\n",
    "\n",
    "Let's disect this **prompt template**:\n",
    "* A single user/model interaction is contained between the `<s>` and `</s>` tags.\n",
    "* The user part of the user/model interaction is contained between the `[INST]` and `[/INST]` tags.\n",
    "* The model part of the user/model interaction follows the `[/INST]` tag and ends at the interaction-concluding `</s>` tag.\n",
    "\n",
    "This template is used during **instruction fine-tuning** and we can use it to provide our own instructive examples to the model for how it ought to behave. We shall make our work easier by creating a helper function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ud-OmX4gYktx"
   },
   "source": [
    "#### A Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "dSa6fN06Sap7",
    "outputId": "0e455c98-4f0e-49e6-dc7a-f034de65aa9d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prompt_with_examples(prompt, examples=[]):\n",
    "    \"\"\"\n",
    "    This function takes an initial prompt and a list of example prompt/response pairs, then\n",
    "    formats them into a single string according to the model's prompt template.\n",
    "    Each example is included in the final prompt, which could be beneficial for models that\n",
    "    take into account the context provided by examples.\n",
    "\n",
    "    Parameters:\n",
    "    @param prompt (str): The main prompt to be processed by the language model.\n",
    "    @param examples (list of tuples): A list where each tuple contains a pair of strings\n",
    "      (example_prompt, example_response). Default is an empty list.\n",
    "\n",
    "    Returns:\n",
    "    str: A string with the structured prompt and examples formatted for a language model.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    main_prompt = \"Translate the following sentence into French:\"\n",
    "    example_pairs = [(\"Hello, how are you?\", \"Bonjour, comment ça va?\"),\n",
    "                     (\"Thank you very much!\", \"Merci beaucoup!\")]\n",
    "    formatted_prompt = prompt_with_examples(main_prompt, example_pairs)\n",
    "    print(formatted_prompt)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    # Start with the initial part of the prompt\n",
    "    full_prompt = \"<s>[INST]\\n\"\n",
    "\n",
    "    # Add each example to the prompt\n",
    "    for example_prompt, example_response in examples:\n",
    "        full_prompt += f\"{example_prompt} [/INST] {example_response} </s><s>[INST]\"\n",
    "\n",
    "    # Add the main prompt and close the template\n",
    "    full_prompt += f\"{prompt} [/INST]\"\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0A_yVsGYmRB"
   },
   "source": [
    "#### One-Shot Learning Example\n",
    "\n",
    "Let's briefly step away from our sentiment analysis task, and use a simple text generation prompt to explore how we can use the `prompt_with_examples` function to provide an instructive example, or put another way, perform **one-shot learning**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "klNQAp-EYpa_",
    "outputId": "d95a56de-c64e-421b-b3d2-f4d91fae2b6a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_prompt = \"Give me an all uppercase color that starts with the letter 'O'.\"\n",
    "example_response = \"ORANGE\"\n",
    "\n",
    "example_1 = (example_prompt, example_response)\n",
    "examples = [example_1]\n",
    "prompt = \"Give me an all uppercase color that starts with the letter 'P'.\"\n",
    "\n",
    "prompt_with_one_example = prompt_with_examples(prompt, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "ec-YGlORY-7k",
    "outputId": "70aadbe4-44b8-49d0-a9e5-31be651d0db2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]\n",
      "Give me an all uppercase color that starts with the letter 'O'. [/INST] ORANGE </s><s>[INST]Give me an all uppercase color that starts with the letter 'P'. [/INST]\n"
     ]
    }
   ],
   "source": [
    "print(prompt_with_one_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JroM5DJZZJuW"
   },
   "source": [
    "`prompt_with_one_example` above includes a single user/model interaction (`<s>...</s>`), using the LLaMA-2 **prompt template**, prepended to the main prompt. Note that the main prompt only includes the user part of the interaction (between the `[INST]` and `[/INST]` tags) and leaves the rest of the interaction (the model's response and the `</s>` tag) for the model to complete.\n",
    "\n",
    "Before using `prompt_with_one_example` let's see what kind of response we get back from the model passing in just the main prompt, without an instructive example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "awD421KtaeR9",
    "outputId": "64d92e5c-fd00-4453-a460-c842112e3c3c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Sure! Here's an all uppercase color that starts with the letter 'P':\n",
      "\n",
      "PURPLE\n"
     ]
    }
   ],
   "source": [
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUnu9OqmaiGM"
   },
   "source": [
    "Although we got `PURPLE` we also got additional chat-like response prior to the output that we want.\n",
    "\n",
    "Now let's get a response using `prompt_with_one_example`, which contains an example of how the model should response with only the word for the color we are interested in it generating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ig_3klfXaqWm",
    "outputId": "25ad7186-4577-42a1-fba4-803dcd55ce89"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PURPLE\n"
     ]
    }
   ],
   "source": [
    "print(generate(prompt_with_one_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1vUSO1rasHt"
   },
   "source": [
    "#### Sentiment Analysis with Examples\n",
    "\n",
    "Previously we lacked confidence about whether the model would correctly label a neutral review. Let's now apply what we just learned about **one-shot learning** to provide our model with an instructive example of responding to what, as a human, we would consider a neutral review.\n",
    "\n",
    "Here is an example we would like classified as a neutral review, which while clearly not negative contains both positive and negative sentiments about the bike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "lsIOizhxa8Ac",
    "outputId": "6e8231b9-4893-452f-a57f-78190769463b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_neutral_review = \"\"\"\n",
    "I've had the chance to put several miles on my new Nyota Ndogo from Nyota Bicyles.\n",
    "First off, the bike's design is sleek, and it provides an exceptionally stable ride,\n",
    "even when navigating the bustle of city streets. The gear shifting is fluid,\n",
    "and the bike feels robust, promising longevity. On the downside, the braking system,\n",
    "while reliable, lacks the responsiveness I've experienced with other bikes.\n",
    "I also noticed that the handlebar grips can become rather uncomfortable on prolonged journeys.\n",
    "Nevertheless, these issues aside, the bike offers impressive performance for its price range,\n",
    "making it a solid, middle-of-the-road choice for both commuting and leisure rides.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toCrM7PkbKDY"
   },
   "source": [
    "We will use the example review to construct an `examples` list of prompt/response pairss we can pass into the `prompt_with_examples` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "NSw50mW4bX5i",
    "outputId": "0ca2e270-6868-42a0-81bd-893210ceb1a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_prompt_neutral = f\"\"\"\n",
    "What is the overall sentiment of this review {example_neutral_review}?\n",
    "\n",
    "Choose one of \"negative\", \"neutral\", or \"positive\".\n",
    "Overall Sentiment:\n",
    "\"\"\"\n",
    "example_response_neutral = \"neutral\"\n",
    "\n",
    "example_neutral = (example_prompt_neutral, example_response_neutral)\n",
    "examples = [example_neutral]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPx_2mUAbcvg"
   },
   "source": [
    "Now we construct the main prompt, which again uses the review from above that we hope to be classified as neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "nQLhV-g1bdLj",
    "outputId": "30abbedf-c34a-486b-dd06-bcee403ce7f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the overall sentiment of this review {review}?\n",
    "\n",
    "Choose one of \"negative\", \"neutral\", or \"positive\".\n",
    "Overall Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "prompt_with_one_example = prompt_with_examples(prompt, examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "L9KpGcBcbhT3",
    "outputId": "0a367094-0d30-4a8d-b381-15906e054915"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "print(generate(prompt_with_one_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSOsiUEVbjdS"
   },
   "source": [
    "Not the response we were hoping to get since the model still labels the review as `\"Positve\"`. Perhaps providing more examples will guide the model better. Lets give two-shot learning a shot, pun intended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6RaElRUb8cj"
   },
   "source": [
    "#### Two-Shot Learning\n",
    "\n",
    "In addition to the neutral example we have given the model, let's also provide it with an example of a positive review in hopes that it will be more clear about the difference between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "RB4ifa-gbt4r",
    "outputId": "183991d9-6be8-4e14-d7ac-c91f638c69a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_review_positive = \"\"\"\n",
    "I've been absolutely delighted with my Starlight Cruiser purchase from Star Bikes.\n",
    "The bike exudes a charm with its sleek design that turns heads as I glide through city lanes.\n",
    "It's not just about looks though; the bike performs wonderfully. The gears shift like a dream,\n",
    "making for a ride that's as smooth as silk across various urban terrains. I was initially skeptical\n",
    "about the comfort of the seat, but it proved to be pleasantly supportive, even on my longer weekend adventures.\n",
    "While the color choices were limited, I found one that suited my style perfectly.\n",
    "Any minor imperfections pale in comparison to the bike's overall quality and the sheer joy it brings to my daily commutes.\n",
    "For the price, the Starlight Cruiser is an undeniable gem that I would happily recommend.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "6je53mgvcMBY",
    "outputId": "53ab6616-923e-4a0a-fc32-84b78b5ee007"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_prompt_positive = f\"\"\"\n",
    "What is the overall sentiment of this review {example_review_positive}?\n",
    "\n",
    "Choose one of \"negative\", \"neutral\", or \"positive\".\n",
    "Overall Sentiment:\n",
    "\"\"\"\n",
    "example_response_positive = \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "8AlWBDUBcN-s",
    "outputId": "735d3916-ac6a-4fdb-8380-5c38cb1c96d3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main_prompt = f\"\"\"\n",
    "What is the overall sentiment of this review {review}?\n",
    "\n",
    "Choose one of \"negative\", \"neutral\", or \"positive\".\n",
    "Overall Sentiment:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLKDDPVEcQY-"
   },
   "source": [
    "#### Exercise: Perform Two-Shot Learning\n",
    "\n",
    "Perform **two-shot learning** by providing the model with both a neutral and a positive example interaction before prompting it for a response to `review` which we are hoping the model will classify as `neutral`.\n",
    "\n",
    "- Use `example_neutral` (already defined above) as one example.\n",
    "- Use `example_review_positive`, `example_prompt_positive` and `example_response_positive` above to construct a positive user/model interaction example. Call it `example_positive`.\n",
    "- Use both examples (neutral and positive) along with `main_prompt` above, to construct a prompt with two examples (using the `prompt_with_examples` function).\n",
    "- Generate and print a model response using your prompt with two examples.\n",
    "\n",
    "If you get stuck, there is a working solution below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VhCPC29dS03"
   },
   "source": [
    "#### Your work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "mX1nr7dJdLW8",
    "outputId": "e0888000-3ec5-4443-880c-fe34012837e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n"
     ]
    }
   ],
   "source": [
    "example_positive = (example_prompt_positive, example_response_positive)\n",
    "examples = [example_neutral, example_positive]\n",
    "\n",
    "prompt_with_two_examples = prompt_with_examples(main_prompt, examples)\n",
    "\n",
    "print(generate(prompt_with_two_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ROpY2GcdVMS"
   },
   "source": [
    "#### Solution\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Click here to see the solution\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "example_positive = (example_prompt_positive, example_response_positive)\n",
    "examples = [example_neutral, example_positive]\n",
    "\n",
    "prompt_with_two_examples = prompt_with_examples(main_prompt, examples)\n",
    "\n",
    "print(generate(prompt_with_two_examples))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3aKPM3lduz-"
   },
   "source": [
    "## Additional Task (Optional)\n",
    "### Generating Data for Downstream Consumption\n",
    "\n",
    "Now that our model is able to perform **sentiment analysis** effectively, let's extend its analysis capabilities to be able to generate JSON objects for downstream consumption that contain a given review's positive and negative points.\n",
    "\n",
    "We will begin iterating on a prompt by simply asking the model to separately list out the positive and negative points in a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "gnRjM7OkdsOL",
    "outputId": "47599798-8736-449a-f92e-6ebcb9acb1ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive points:\n",
      "\n",
      "* Smooth ride\n",
      "* Handles urban terrains with ease\n",
      "* Good value for the money\n",
      "\n",
      "Negative points:\n",
      "\n",
      "* Seat can be uncomfortable for longer rides\n",
      "* Limited color options\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "From the review, list the positive points and negative points separately: {review}\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_Tf7dBAgvli"
   },
   "source": [
    "Not surprisingly, the model did quite well. Let's iterate the prompt to get the model to produce a JSON object for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "id": "-l1W3WORg-EU",
    "outputId": "f3f479d9-760a-4852-841d-50fe5ae0e35f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the JSON representation of the review:\n",
      "\n",
      "{\n",
      "\"positive_points\": [\n",
      "\"smooth ride\",\n",
      "\"ease of handling urban terrain\",\n",
      "\"good value for money\"\n",
      "],\n",
      "\"negative_points\": [\n",
      "\"seat uncomfortable for longer rides\",\n",
      "\"limited color options\"\n",
      "]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "From the review, list down the positive points and negative points separately, in JSON: {review}\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfoZnACmhHED"
   },
   "source": [
    "That appears to have made no difference. Let's try to be more **precise** in our prompt about how we want the output to be formatted. We must use double curly braces `{{` and `}}` rather than single ones because we are using Python f-string literals which interprets single curly braces as placeholders for Python variables.\n",
    "\n",
    "**NOTE:** In some instances, the output does look like a JSON output but in reality is not a valid JSON string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "PRAcbvGXhmQK",
    "outputId": "7999e4cb-a262-41af-c30a-3132fec3d630"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive points:\n",
      "\n",
      "* Smooth ride\n",
      "* Handles urban terrains with ease\n",
      "* Good value for the money\n",
      "\n",
      "Negative points:\n",
      "\n",
      "* Seat can be uncomfortable for longer rides\n",
      "* Limited color options\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "From the review below, list down the positive points and negative points separately, in JSON. Use the following format:\n",
    "\n",
    "{{\"positive\": [], \"negative\": []}}\n",
    "\n",
    "Review: {review}\n",
    "\"\"\"\n",
    "\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3inntZchpDy"
   },
   "source": [
    "That did not make any difference either.\n",
    "\n",
    "To output a JSON response, we need to employ the methods we have learnt so far. We can do so in two ways:\n",
    "* Providing the model with **instructive examples** i.e. example reviews alongside their corresponding JSON outputs.\n",
    "* Adding a **cue** to the prompt.\n",
    "\n",
    "We also provide a helper function called `pretty_print_json` which you can pass to the LLM, and it will print the output with nice indenting only if it is valid JSON.\n",
    "\n",
    "If you get stuck, a solution is provided further down below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFuDCwl6i2zL"
   },
   "source": [
    "**1. Using instructive examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "OdFLdBT3iz_V",
    "outputId": "b4a9ef6c-4c8b-4ed4-9e72-02f56cdbce1a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_reviews = [\n",
    "\"\"\"\\\n",
    "I recently purchased the Starlight Cruiser from Star Bikes, and I've been thoroughly impressed. \\\n",
    "The ride is smooth and it handles urban terrains with ease. \\\n",
    "However, I did find the seat a bit uncomfortable for longer rides. \\\n",
    "Also, the color options could be better. Despite these minor drawbacks, \\\n",
    "the build quality and the performance of the bike are commendable. It's a good value for the money.\\\n",
    "\"\"\",\n",
    "\"\"\"\\\n",
    "Got the Starlight Cruiser last week, and I'm a bit disappointed. \\\n",
    "The brakes are not as responsive as I'd like and the gears often get stuck. \\\n",
    "The design is good but performance-wise, it leaves much to be desired.\\\n",
    "\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "gbGHvqKUi_1L",
    "outputId": "afaa40c4-0b2b-4908-bc79-e3517e9c3341"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_outputs = [\n",
    "    {\n",
    "        \"positive\": [\"smooth ride\", \"ease of handling urban terrains\", \"good value for the money\"],\n",
    "        \"negative\": [\"seat uncomfortable for longer rides\", \"limited color options\"]\n",
    "    },\n",
    "    {\n",
    "        \"positive\": [\"good design\"],\n",
    "        \"negative\": [\"brakes not repsonsive\", \"gears often get stuck\", \"performance leaves much to be desired\"]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "F0OIrml5jAiA",
    "outputId": "e0571302-1cb1-41b3-fa4c-3911443276e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pretty_print_json(json_string):\n",
    "    print(json.dumps(json.loads(json_string), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo7sC9pdjFdx"
   },
   "source": [
    "#### Your work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "Y8uA0sfdjHUb",
    "outputId": "897743b6-56b2-415a-fa81-2bacf0b9e693"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            pre {\n",
       "                white-space: pre-wrap;\n",
       "            }\n",
       "        </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"positive\": [\n",
      "        \"smooth ride\",\n",
      "        \"ease of handling urban terrains\",\n",
      "        \"good value for the money\"\n",
      "    ],\n",
      "    \"negative\": [\n",
      "        \"seat uncomfortable for longer rides\",\n",
      "        \"limited color options\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "examples = [(example_review, json.dumps(example_output)) for example_review, example_output in zip(example_reviews, example_outputs)]\n",
    "\n",
    "# we use `review` as our base prompt\n",
    "prompt = prompt_with_examples(review, examples)\n",
    "pretty_print_json(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwbvdRa3F8jk"
   },
   "source": [
    "Now the output is a valid JSON string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mU3hjYDjJEc"
   },
   "source": [
    "#### Solution\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "Click here to see the solution\n",
    "</summary>\n",
    "\n",
    "```python\n",
    "examples = [(example_review, json.dumps(example_output)) for example_review, example_output in zip(example_reviews, example_outputs)]\n",
    "\n",
    "# we use `review` as our base prompt\n",
    "prompt = prompt_with_examples(review, examples)\n",
    "pretty_print_json(generate(prompt))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHLyqYhLkDQb"
   },
   "source": [
    "**2. Adding a cue to the prompt**\n",
    "\n",
    "While Solution **1** demonstrates an effective use of **two-shot learning**, in line with this notebook's objectives, it's worth mentioning, one can get a working solution by adding a **cue** of `JSON:` to the prompt we were iterating on earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G0LkfAajeeS"
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "From the review below, list down the positive points and negative points separately, in JSON. Use the following format:\n",
    "\n",
    "{{\"positive\": [], \"negative\": []}}\n",
    "\n",
    "Review: {review}\n",
    "JSON output:\n",
    "\"\"\"\n",
    "\n",
    "pretty_print_json(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLA6xeqzkXoI"
   },
   "source": [
    "### Review of Main Concepts\n",
    "\n",
    "The following key concepts were introduced in this lab:\n",
    "\n",
    "- **Precise**: Being as explicit as necessary to guide the response of an LLM.\n",
    "- **Cue**: A conclusion to a prompt that guides its response, often to prevent it from including the cue itself in its response.\n",
    "- **\"Time to think\"**: A quality in prompts that supports LLM responses (often requiring calculation) by asking for the model to take multiple steps and show its work.\n",
    "- **Sentiment Analysis:** Identifying the mood or sentiment for a piece of text.\n",
    "- **Instruction Fine-Tuning:** Improving a model's task performance through tailored example-based learning.\n",
    "- **LLaMA-2 Prompt Template:** A pre-designed format guiding LLaMA-2 model responses, used during instruction fine-tuning.\n",
    "- **Few-shot Learning:** Prepending one-to-many instructive examples to a model to improve its responses."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
